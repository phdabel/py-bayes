Em matemática e análise numérica, o processo de Gram-Schmidt é um método para ortogonalização de um conjunto de vetores em um espaço com produto interno, normalmente o espaço Euclidiano Rn. O processo de Gram–Schmidt recebe um conjunto finito, linearmente independente de vetores S = {v1, …, vn} e retorna um conjunto ortogonal S' = {u1, …, un} que gera o mesmo subespaço S inicial.O método leva o nome de Jørgen Pedersen Gram e Erhard Schmidt mas pode ser encontrado antes nos trabalhos de Laplace e Cauchy. Na teoria da Lie group decompositions é generalizado pela decomposição de Iwasawa.O processo se dá da seguinte maneira:A seqüencia u1, …, uk é o sistema de vetores ortogonais e (não necessariamente) normalizados e1, …, ek formam um conjunto ortonormal, ou seja: de vetores ortogonais entre si dois a dois e normalizados.Para verificar que estas fórmulas resultam em uma seqüencia ortogonal, primeiro compute 〈u1, u2〉 substituindo a fórmula à cima por u2: encontra-se 0 (zero). Então usando esse fato para computar 〈u1, u3〉 novamente substituindo a fórmula por u3: encontra-se 0. A prova geral se processa por indução matemática. O processo de Gram-Schmidt determina uma base de vetores ortonormais que, ao sofrer uma composição linear por uma matriz alternativamente diagonal, se torna equivalente à aplicação linear da base canônica que sofreu o processo.Essa série de passos resulta no mesmo conjunto do processo original, porém introduz menos erro em uma aritmética de precisão finita.